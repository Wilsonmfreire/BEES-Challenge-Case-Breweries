pandas
pyspark
python 13.12.7

# Configuração do Ambiente

Este projeto requer as seguintes variáveis de ambiente:

*   `HADOOP_HOME`: Caminho para a instalação do Hadoop (ex: `C:/hadoop` no Windows ou `/usr/local/hadoop` no Linux/macOS).
*   `SPARK_HOME`: Caminho para a instalação do Spark (ex: `C:/spark` ou `/usr/local/spark`).

**Passos para configuração:**

1.  Instale o Hadoop e o Spark na sua máquina.
2.  Defina as variáveis de ambiente `HADOOP_HOME` e `SPARK_HOME` apontando para os diretórios de instalação.
    *   **Windows:** Use as configurações de "Variáveis de Ambiente" do sistema.
    *   **Linux/macOS:** Adicione as variáveis ao arquivo `~/.bashrc` ou `~/.zshrc` (ex: `export HADOOP_HOME=/usr/local/hadoop`).
3.  Reinicie o terminal ou o computador para que as alterações nas variáveis de ambiente sejam aplicadas.
