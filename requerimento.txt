Environment Configuration
pandas

pyspark

python 13.12.7 This project requires the following environment variables:

HADOOP_HOME: Path to Hadoop Installation (ex: C:/hadoop on Windows or /usr/local/hadoop on Linux/macOS).

SPARK_HOME: Path to Spark installation (ex: C:/spark or /usr/local/spark).

Steps for configuration:

Install Hadoop and Spark on your machine.
Set the HADOOP_HOME and SPARK_HOME environment variables pointing to the installation directories.
Windows: Use the system's "Environment Variables" settings.
Linux/macOS: Add the variables to the ~/.bashrc or ~/.zshrc file (ex: export HADOOP_HOME=/usr/local/hadoop).
Restart the terminal or computer for the changes to the environment variables to take effect.
